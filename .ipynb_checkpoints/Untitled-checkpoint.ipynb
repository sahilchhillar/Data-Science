{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indian-advantage",
   "metadata": {},
   "source": [
    "This cell is to import the necessary libraries for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mobile-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-parts",
   "metadata": {},
   "source": [
    "This cell creates a custom dataset for the images and the files\n",
    "\n",
    "* It uses Dataset library of pytorch and creates a class that has the following methods\n",
    "  1. __init__() -> This is a constructor of the class that is used to load the necessary files in the class\n",
    "  2. __len__() -> This method is used to return the length of the dataset\n",
    "  3. __getitem__() -> This methos is used to load a particular file from the big fat dataset that we have. It also applies necessary transformations to the data if required, in our case we need to conver the images to tensors and we can do that here.\n",
    "  \n",
    "  Custom dataset __init__() method takes 3 arguments, label_file, image_path, transforms. \n",
    "  1. label_file contains the csv file that has the names of the images and the associated type of cancer.\n",
    "  2. image_path contains the path to the image folder.\n",
    "  3. transform contains any kind of transformations that can be used on the data\n",
    "  \n",
    "  Custom dataset __len__() method returns the length of the dataset which is the length of the label_file.\n",
    "  \n",
    "  Custom dataset __geitem__() method takes one argument which is the index of the data. It then: \n",
    "  1. Finds the image name from the label_file and then finds out the path of the image.\n",
    "  2. It then read the image file and convert the data to float.\n",
    "  3. It then normalises the data by dividing it by 255.0 and convert it into a numpy array and reshape to work on the data.\n",
    "  4. We find the label form the label_file and encode the label based on the type of cancer and store it as a numpy array.\n",
    "  5. Then we apply transformation on the image and label (if any).\n",
    "  \n",
    "  We also create a ToTensor class which transforms the data from numpy to tensor and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rural-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, label_file, image_path, transform=None):\n",
    "        self.labels = pd.read_csv(label_file)\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_path, f'{self.labels.iloc[index, 0]}.png')\n",
    "        image = io.imread(img_path).astype('float32')\n",
    "        image /= 255.0\n",
    "        image = np.array(image)\n",
    "        image = image.reshape(3, 64, 64)\n",
    "\n",
    "        label = self.labels.iloc[index, 1]\n",
    "        if label == ' Connective':\n",
    "            label = 0\n",
    "        elif label == ' Immune':\n",
    "            label = 1\n",
    "        elif label == ' Cancer':\n",
    "            label = 2\n",
    "        else:\n",
    "            label = 3\n",
    "        label = np.array(label)\n",
    "\n",
    "        sample = (image, label)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supreme-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[0], sample[1]\n",
    "        image = torch.from_numpy(image)\n",
    "        label = torch.from_numpy(label)\n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-irish",
   "metadata": {},
   "source": [
    "Creating an object to load the dataset and apply transformation by sending ToTensor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "purple-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "image_dataset = ImageDataset(label_file='train.csv', image_path='train/', transform=transforms.Compose([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "driven-fantasy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190\n",
      "torch.Size([3, 64, 64]) torch.Size([])\n",
      "torch.Size([3, 64, 64]) torch.Size([])\n",
      "torch.Size([3, 64, 64]) torch.Size([])\n",
      "torch.Size([3, 64, 64]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(len(image_dataset))\n",
    "\n",
    "for i in range(4):\n",
    "    sample = image_dataset[i]\n",
    "    print(sample[0].shape, sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-designation",
   "metadata": {},
   "source": [
    "Here we are creating a DataLoader object which is used to load the dataset. \n",
    "\n",
    "* batch_size is used to create mini batches of the data because working on a very large dataset is not efficient and the model may get stuck in local minimum.\n",
    "* shuffle is just used to shuffle the dataset, so that the batches have different values everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "handmade-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "1 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "2 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "3 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "4 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "5 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "6 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "7 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "8 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "9 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "10 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "11 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "12 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "13 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "14 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "15 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "16 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "17 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "18 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "19 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "20 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "21 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "22 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "23 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "24 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "25 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "26 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "27 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "28 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "29 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "30 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "31 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "32 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "33 torch.Size([64, 3, 64, 64]) torch.Size([64])\n",
      "34 torch.Size([14, 3, 64, 64]) torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=image_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "for i, (image, label) in enumerate(dataloader):\n",
    "    print(i, image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-contact",
   "metadata": {},
   "source": [
    "This is the main model where all the things happen.\n",
    "It has 2 methods mainly, __init__() and the forward()\n",
    "\n",
    "* __init__() -> this method is used to create all the convolutional or linear layers that we want. In the code below, we have 2 convolutional layers and 3 linear layers.\n",
    "\n",
    "    1. First convolutional layer has 3 in channels because of the channels of a single image is 1. It then has 32 out channels, kernel_size of 3 which creates a 3x3 filter, stride of 1 which is used to identify the speed of the filter, having 1, it will not skip any pixel and the filter will be checked on every 3x3 pixel of the image, padding 1 is applied to allow kernel to con=ver more spaces.\n",
    "    2. Then we have an activation function that will find the value that comes out from the filter on that particular pixel values.\n",
    "    3. Then we have MaxPool function that will create a 2x2 matrix of activation values and find out the maximum from that matrix.\n",
    "    4. Then we have another convolutional layer.\n",
    "    5. Then we flatten the image to create a single long feature vector.\n",
    "    6. Then we have added 3 linear layers and the output of the layers is 4 because at the end we have just 4 labels.\n",
    "    \n",
    "    \n",
    "* forward() -> This method is used to apply the layers on the images. It has some activation functions between every layer to calculate the value of the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "offshore-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16384, 5000),\n",
    "            nn.Linear(5000, 3000),\n",
    "            nn.Linear(3000, )\n",
    "            nn.Linear(1000, 4)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-world",
   "metadata": {},
   "source": [
    "Here we create a method of the neural network.\n",
    "\n",
    "We create an optimisation function which will calculate the weights and bias of layers, we have used SGD as the optimisation function.\n",
    "\n",
    "We have a loss function to calculate the loss for the layer, we have used CrossEntropyLoss because we need to calulate multiple class values and this is the most famous and widely used loss funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "separate-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=16384, out_features=5000, bias=True)\n",
      "    (8): Linear(in_features=5000, out_features=1000, bias=True)\n",
      "    (9): Linear(in_features=1000, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-garbage",
   "metadata": {},
   "source": [
    "The below code is to calculate the loss for every mini batch. It takes one argument (epochs) which is just the number of iterations. Then a variable that will store the values of the losses.\n",
    "\n",
    "* It the runs on every batch, train the model.\n",
    "* optimizer.zer_grad() is used to initalize the optimizer value to 0 in case we have any previous value.\n",
    "* Then we just find the output for the images of that batch.\n",
    "* Then we calculate the loss.\n",
    "* We go step backwards to minimise the loss.\n",
    "* Minimising the loss is done by optimizer.step() which changes the value of weights and bias in order to minimise the loss.\n",
    "* Then we add the loss value for every batch and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serial-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Hello\")\n",
    "        epoch_loss = 0.0\n",
    "        for i, (image, label) in enumerate(dataloader):\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = net(image)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_loss.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch} has a loss of {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-theta",
   "metadata": {},
   "source": [
    "Calling the training model 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "capable-target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Epoch 0 has a loss of 46.37416696548462\n",
      "Hello\n",
      "Epoch 1 has a loss of 44.57157349586487\n",
      "Hello\n",
      "Epoch 2 has a loss of 42.4744678735733\n",
      "Hello\n",
      "Epoch 3 has a loss of 38.00683629512787\n",
      "Hello\n",
      "Epoch 4 has a loss of 36.15666860342026\n",
      "Hello\n",
      "Epoch 5 has a loss of 34.784969449043274\n",
      "Hello\n",
      "Epoch 6 has a loss of 39.17500287294388\n",
      "Hello\n",
      "Epoch 7 has a loss of 33.99421763420105\n",
      "Hello\n",
      "Epoch 8 has a loss of 32.24447625875473\n",
      "Hello\n",
      "Epoch 9 has a loss of 30.50210678577423\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "golden-cylinder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa6b1de5fa0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoiElEQVR4nO3dd3RUdf7G8fcnjSSU0EKRACF0CEWNFIEgSFVE7N21AO7aUFxxdXX52csqYEMF1LVgWwVElKYCCZ3QS+i9JoD0Tr6/PxJcVDADZHJnkud1Ts7JTObmPjtHnv3mzr33Y845REQk+IR4HUBERM6OClxEJEipwEVEgpQKXEQkSKnARUSCVFh+7qxs2bIuPj4+P3cpIhL0Zs+evd05F/v75/O1wOPj40lLS8vPXYqIBD0zW3eq53UIRUQkSKnARUSClApcRCRIqcBFRIKUClxEJEipwEVEgpQKXEQkSAVFgX83fzMj5m5Ct74VEfmfoCjwYXM28tCX8+jx8Wwy9hzyOo6ISEAIigIf8peLePLyuqSuyKR9/xSGzdmo1biIFHo+F7iZhZrZXDMblfPYzOx5M1tuZulm9qC/QoaGGN1bJTC6VytqlitG76/m0/2jNLZpNS4ihdiZrMB7AeknPb4DqAzUcc7VBb7Iw1ynlBBbjC/vac5TXeoxZdV22vebxNeztRoXkcLJpwI3szjgcmDISU//DXjGOZcF4JzLyPt4fxQaYtzdshqjeyVTu0Jx/v7f+dz1n1ls3a3VuIgULr6uwAcAfYCsk56rDtxgZmlmNtrMap5qQzPrmfOatMzMzHNLe5JqZYvyZc/m9L2iHtNW76B9/0l8lbZBq3ERKTRyLXAz6wJkOOdm/+5HRYBDzrkkYDDwwam2d84Ncs4lOeeSYmP/cDvbcxISYtzZohpjeiVTt2IJ+ny9gDs+nMXmXQfzdD8iIoHIlxV4C6Crma0l+zh3WzP7FNgIDMt5zXCgoV8S+iC+bFG+6NGMp7vWZ+aanXTsn8KXs9ZrNS4iBVquBe6ce9w5F+eciwduBH52zt0KjADa5LysNbDcXyF9ERJi/OXieMY+lEz9SiV47JuF3P7BTDZpNS4iBdS5nAf+EnCNmS0EXgS6502kc1OlTDSfdW/Gs1fWZ/a6X+jYP4XPZmg1LiIFj+VnsSUlJbn8HKm2YecBHvtmAVNX7aBljbK8dE0D4kpF59v+RUTygpnNzvm88TeC4krMs1W5dDRDuzfl+asSmbs+ezX+6fR1Wo2LSIFQoAscwMy4pWlVxj6czPlVSvHkiEXcMmQGG3Ye8DqaiMg5KfAFfkJcqWg+ubsJL17dgAUbd9NxQAqfTFtLVpZW4yISnApNgUP2avymJlUY+3AyF1YtxVPfLubmIdNZv0OrcREJPoWqwE+oVDKKj+9qwsvXNGDxpj10HJDCR1O1GheR4FIoCxyyV+M3XJS9Gm9SrTR9Ry7mxsHTWbdjv9fRRER8UmgL/ITzSkbxnzsv4pVrG5K+JXs1/uGUNVqNi0jAK/QFDtmr8euTKjPu4WSaJ5Th6e+WcOOg6azZrtW4iAQuFfhJKsZE8cEdF/HqdY1I37qHzq+nMCR1Nce1GheRAKQC/x0z49oL4xj/cGtaVC/Lc9+nc/1701iduc/raCIiv6ECP40KMZEM+UsS/a5vxIpte+n8eiqDU7QaF5HAoQL/E2bG1RfE8WPv1rSqGcvzP6Rz8+DpZOzV9B8R8Z4K3AflSkQy+PYLefW6RszfuIsub0xm1tqdXscSkUJOBe6jE8fGR9zXguiIUG4cNJ0hqat1YywR8YwK/AzVqVCCkQ+0pF3dcjz3fTr3Dp3D3kNHvY4lIoWQCvwslIgM591bL+SJy+owbsk2rnxrCsu27vU6logUMj4XuJmFmtlcMxv1u+ffMLNCd46dmdEzuTqfdW/K3sPH6Pb2FEbM3eR1LBEpRM5kBd4LSD/5CTNLAkrlaaIg0zShDN8/0JIGlWJ46Mt5PDViEYePHfc6logUAj4VuJnFAZcDQ056LhT4N9DHP9GCR7kSkQzt0ZSeyQl8Mn0d1783XcOURcTvfF2BDyC7qLNOeu5+YKRzbsufbWhmPc0szczSMjMzzy5lEAgPDeGJy+ry7q0XsCpjH13eSGXS8oL7v1dEvJdrgZtZFyDDOTf7pOfOA64D3sxte+fcIOdcknMuKTY29pzCBoNOiRUZeX8LypeI5I4PZ/L6jyt0Z0MR8QtfVuAtgK5mthb4AmgLLAZqACtzno82s5X+ChlsEmKLMfzeFlzVuBL9f1zOnf+ZxS/7j3gdS0QKmFwL3Dn3uHMuzjkXD9wI/OycK+Wcq+Cci895/oBzroafswaVqIhQXru+Ec9flci0VTvo8uZk5m/Y5XUsESlAdB64H5kZtzStytd/aw7Ade9OY+iMdbp6U0TyxBkVuHNuonOuyymeL5Z3kQqehnElGfVAS5pXL8M/hy/ika/mc/CITjUUkXOjFXg+KVU0gg/vuIiH29Vi+LxNXDVwiib+iMg5UYHno5AQo1e7mvznziZs3XOIrm9OZsyiPz0LU0TktFTgHmhdK5bvH2xFQrli/PXTObzwQzrHjmflvqGIyElU4B6pVDKKr+5pxm3NqjIoZTU3D5lBxh4NihAR36nAPVQkLJRnuyUy4IbGLNy4m8vfnMyM1Tu8jiUiQUIFHgC6nV+JEfe1oHiRMG4eMoNBKat0qqGI5EoFHiBqVyjOt/e3oEO98rzww1L+9ukc9mhQhIj8CRV4ACkeGc7AWy7gycvrMj59G13fnEz6lj1exxKRAKUCDzBmRvdWCXzeoxkHjhznqoFTGDZno9exRCQAqcADVJNqpRn1YEsaVy5J76/m88/hCzUoQkR+QwUewMoVj+TTu5vy19bVGTpjPde9O40NOw94HUtEAoQKPMCFhYbwj851eO+2C1mTuZ8r3prMxGUZXscSkQCgAg8SHetX4LsHWlKhRCR3/mcW383f7HUkEfGYCjyIxJctyvB7W9AwriRPf7eY3Qd0mqFIYaYCDzJREaE83y2RnfuP8Oq4ZV7HEREPqcCDUGKlGG5vHs+nM9axYOMur+OIiEd8LnAzCzWzuWY2KufxUDNbZmaLzOwDMwv3X0z5vd4dalGmaBGeGrGI4xqaLFIonckKvBeQftLjoUAdoAEQBXTPw1ySixKR4TzVpS7zN+7m85nrvY4jIh7wqcDNLA64HBhy4jnn3A8uBzATiPNPRDmdro3Oo3lCGf49dhnb9x32Oo6I5DNfV+ADgD7AH6YO5Bw6uQ0Yc6oNzaynmaWZWVpmZubZ5pRTMDOe7Vaf/YeP8dLopV7HEZF8lmuBm1kXIMM5N/s0LxkIpDjnUk/1Q+fcIOdcknMuKTY29hyiyqnUKFec7q0S+Hr2Rmat3el1HBHJR76swFsAXc1sLfAF0NbMPgUws75ALNDbbwklVw9eWoPzYiJ5cvgijmo0m0ihkWuBO+ced87FOefigRuBn51zt5pZd6AjcJNzTq3hoeiIMPp2rc+ybXv5aOpar+OISD45l/PA3wXKA9PMbJ6Z/SuPMslZ6FCvPG1qx9J//HK27tZsTZHC4IwK3Dk30TnXJef7MOdcdedc45yvZ/wTUXxhZjzdNZFjWY5nv1/idRwRyQe6ErMAqVImmnsvqcH3C7aQslxn/IgUdCrwAuae1gnEl4mm78jFGgAhUsCpwAuYyPBQnrkykTXb9zNo0mqv44iIH6nAC6DkWrFc1qACb01YqQk+IgWYCryAeqpLPUJDjL4jF5N9twMRKWhU4AVUxZgoHm5Xi5+XZjB+yTav44iIH6jAC7A7WsRTq3wxnv5uCQeOHPM6jojkMRV4ARYeGsJz3RqwaddB3vp5pddxRCSPqcALuCbVSnP1BZUYnLqalRn7vI4jInlIBV4IPN65LlHhofzr20X6QFOkAFGBFwKxxYvwaKc6TF21g5HzN3sdR0TyiAq8kLi5SRUaxsXw3Pfp7Dl01Os4IpIHVOCFRGiI8eyViWzfd5j+45d7HUdE8oAKvBBpVLkkNzepwkdT17J4826v44jIOVKBFzJ9OtahVHQET41YRFaWPtAUCWY+F7iZhZrZXDMblfO4mpnNMLOVZvalmUX4L6bklZjocB6/rC5z1u/iv7M3eB1HRM7BmazAewHpJz1+GejvnKsB/ALcnZfBxH+uuaASF8WX4qXRS/ll/xGv44jIWfKpwM0sDrgcGJLz2IC2wNc5L/kI6OaHfOIHZsaz3RLZc+gYr4xd6nUcETlLvq7ABwB9gBPDi8sAu5xzJ26wsRGolLfRxJ/qVCjBnRfH8/nMDcxZ/4vXcUTkLORa4GbWBchwzs0+mx2YWU8zSzOztMxMjfkKJA+1r0X5EkV4asQijh3Pyn0DEQkovqzAWwBdzWwt8AXZh05eB0qaWVjOa+KATafa2Dk3yDmX5JxLio2NzYPIkleKFQnjX13qs3jzHj6dvs7rOCJyhnItcOfc4865OOdcPHAj8LNz7hZgAnBtzsv+Anzrt5TiN5c1qECrmmV5bdxyMvYe8jqOiJyBczkP/DGgt5mtJPuY+Pt5E0nyk5nxdNf6HD6WxQvfp+e+gYgEjDMqcOfcROdcl5zvVzvnmjjnajjnrnPOHfZPRPG3hNhi3NM6gRHzNjN11Xav44iIj3QlpgBwX5saVC4dxb++XcyRY/pAUyQYqMAFgMjwUP7vivqszNjH+5PXeB1HRHygApdfXVq3PO3rleeNn1awaddBr+OISC5U4PIbfa+oh8PxzHeLvY4iIrlQgctvxJWK5oG2NRm7eBsTlmZ4HUfO0Is/pDM4ZbXXMSSfqMDlD3q0SqB6bFH6jlzMoaPHvY4jPhq/ZBvvpazm5TFLWb/jgNdxJB+owOUPIsJCePbKRNbvPMDAiau8jiM+2H/4GH2/XURC2aKEhRr9f9TUpcJABS6ndHGNsnRtdB7vTlrFmu37vY4juXjjpxVs3n2IV65tyF8ujmfEvE0s27rX61jiZypwOa0nL69LRGgIfUcuxjlN7wlU6Vv2MGTyGm68qDJJ8aX5a3J1ikWE8dq4ZV5HEz9TgctplSsRSe/2tUhZnsnoRVu9jiOnkJXl+OfwhcREhfNYpzoAlCoaQY/kBMYt2ca8Dbu8DSh+pQKXP3V786rUrViCZ75bwr7Dx3LfQPLVl2kbmLN+F09cVpdSRf831fCultUoUzSCV8dqFV6QqcDlT4WFhvBct0S27jnEGz+t8DqOnGT7vsO8NHopTauV5poLfjtPpViRMO5tU4PJK7czdaXub1NQqcAlVxdWLcUNSZX5YPIafTAWQF74Pp0DR47x/FWJZE85/K1bmlahYkwkr4xdps8wCigVuPjksc51KBYZxlMjFqkMAsDUVdsZNncT9yRXp0a54qd8TWR4KL0urcm8Dbv4MV0XZRVEKnDxSemiETzWqQ4z1+5k2JxTDl+SfHL42HGeHL6IKqWjub9tjT997TUXxlGtbFFeHbuMrCz9H29BowIXn92QVJnGlUvy4uh0dh846nWcQuu9SatZvX0/z1xZn8jw0D99bXhoCA+3r8WybXv5bsHmfEoo+cWXocaRZjbTzOab2WIzezrn+UvNbI6ZzTOzyWb250sBCXohIcZz3RLZuf8Ir+ocY0+s3b6ftyas5PKGFbmkdjmftunSoCJ1K5ag3/jlHNXw6gLFlxX4YaCtc64R0BjoZGbNgHeAW5xzjYHPgCf9FVICR2KlGG5vHs+nM9axcONur+MUKs45nvp2EUVCQ/hXl3o+bxcSYjzasRbrdhzgq7QNfkwo+c2XocbOObcv52F4zpfL+SqR83wMoL/PConeHWpRpmgRnhyxkOM6rppvvluwhdQV2/l7x9qULxF5Rtu2qV2OC6uW4o2fVugGZQWIT8fAzSzUzOYBGcB459wMoDvwg5ltBG4DXvJbSgkoJSLDeapLXeZv3M0N701jre6V4ne7Dx7l2VFLaBgXw63Nqp7x9mbGox1rs23PYT6Zts4PCcULPhW4c+54zqGSOKCJmSUCDwOXOefigA+Bfqfa1sx6mlmamaVlZmbmUWzx2pWNK9H/hkYs27aXzq+n8sm0tTq90I9eHbuMHfsO83y3BoSG/PGcb180SyhDq5plGThxJXsP6UPoguBMp9LvAiYAnYFGOStxgC+Bi0+zzSDnXJJzLik2NvZcskqAuer8OMY9nExSfCme+nYxt38wk80axZbn5m3Yxacz1nF783gaxMWc0+/q07EOvxw4qrmnBYQvZ6HEmlnJnO+jgPZAOhBjZrVyXnbiOSlkKsZE8fFdTXiuWyKz1/1Cx/4pfD17o1bjeeTY8Sz+OXwh5YoX4ZEOtXLfIBcN4mLonFiBIalr2Ln/SB4kFC/5sgKvCEwwswXALLKPgY8CegDfmNl8so+BP+q/mBLIzIxbm1VldK9W1KlYnL//dz49P5lN5t7DXkcLeh9NW8fizXvoe0V9ikeG58nv7N2+FgeOHOOdiSvz5PeJdyw/V0pJSUkuLS0t3/Yn+e94luODyWv497hlFCsSxvPdEuncoKLXsYLSlt0HaffaJC6qVpoP77jolPc7OVuPfDWf7xZsZtKjl1AxJirPfq/4h5nNds4l/f55XYkpeSo0xOiRnMD3D7SkUsko/jZ0Dr2+mKsrN8/CM98t4ViW45mup75Z1bl4qF1NnHO8+bNW4cFMBS5+UbN8cYbdezEPt6vF9wu20GHAJCYs0w2VfPXz0m2MXrSVBy+tSZUy0Xn++yuXjuamJlX4atYGnQYaxFTg4jfhoSH0aleT4fe2ICYqnDs/nMXjwxZqMEQuDh45zlMjFlOjXDF6tErw237ub1uDsFBjgAYgBy0VuPhdg7gYRt7fkntaJ/DFrPV0GpDC9NU7vI4VsN74eQWbdh3k+W6JRIT5759oueKR3NmiGt/O38zSrXv8th/xHxW45IvI8FAe71yX/97TnNAQ46bB03l21BJd1v07y7buZXDKaq67MI6mCWX8vr97khMoViSMV8dqFR6MVOCSr5LiSzO6VytubVqV9yev4fI3UpmvwbtA9oDiJ0cspFhkGI9fVjdf9lkyOoJ7khP4MX0bc9b/ki/7lLyjApd8Fx0RxrPdEvnk7iYcOHKcq9+ZymvjlnHkWOG+1enXszcya+0vPNG5LqVPGlDsb3e20ADkYKUCF8+0qhnLmIeS6da4Em/+vJJub08ptMdid+w7zAuj02kSX5prL4zL130XLRLGfW1qMHXVDqZoAHJQUYGLp2Kiwnnt+kYMuu1CMvYe4oo3JzNw4spCd5vaF0cvZd+hYzx3VSIhZ3mzqnNxS7MqnKcByEFHBS4BoUP9Cox9KJl2dcvzyphlXPfuVNYUkvOTp6/ewdezN9IjOYFa5U89oNjfioSF8lC7WszfsItxS7Z5kkHOnApcAkaZYkUYeMsFvH5jY1Zm7KPz6yl8NHVtgR7Ge+RYFk+OWERcqSgebFvT0yxXX1CJhLJFeW3cskL3F1CwUoFLQDEzrmxcifG9W9MsoQx9Ry7m1vdnsKmA3qZ2cOpqVmbs49krE4mK+PMBxf4WFhpC7w61WL5tHyPnb/I0i/hGBS4BqXyJSD684yJevLoB8zfsolP/FL5K21Cgjs+u33GAN35aQefECrSp49uAYn+7LLEi9SqWoP/4FYX+rKBgoAKXgGVm3NSkCmMeSqbeeSXo8/UCenycRsbeQ15HO2cnBhSHhRh9r6jvdZxfZQ9Ars36nRqAHAxU4BLwKpeO5vMezXiqSz1SV2ynQ/8URi0I7hnaPyzcyqTlmTzSoTYVYs5sQLG/XVI7loviNQA5GKjAJSiEhBh3t6zG9w+2omrpaO7/bC4PfD6XXQeCb6rM3kNHefq7xdQ/rwS3Nz/zAcX+lj0AuQ4Zew/z0dS1XseRP+HLSLVIM5tpZvPNbLGZPZ3zvJnZ82a23MzSzexB/8eVwq5GuWJ887eLeaR9LUYv3EL7/in8lB5cp729Nm45mfsO88JVDQgLDcw1VJNqpWldK5Z3Jq1ijwYgByxf/us5DLR1zjUCGgOdzKwZcAdQGajjnKsLfOGvkCInCwsN4YFLa/Lt/S0oUzSCuz9K44b3pgXFHQ4XbNzFx9PWcluzqjSqXNLrOH/q7x1qs+vAUYakagByoMq1wF22fTkPw3O+HPA34BnnXFbO63S3fslX9c+L4dv7W9D3inqs3r6fGwdN5+bB05m1dqfX0U7peJbjn8MXUaZYEf7esbbXcXLVIC6GyxpU4P3U1ezYp/mmgcinv9/MLNTM5gEZZA81ngFUB24wszQzG21mp7wKwcx65rwmLTMzM8+Ci0D2FYR3tqhGap82PNWlHsu37eO6d6dx65AZzF4XWEX+ybS1LNy0m391qUeJPBpQ7G+929fi4NHjvDNxlddR5BR8KnDn3HHnXGMgDmhiZolAEeBQzqDNwcAHp9l2kHMuyTmXFBsbm0exRX4rMjyUu1tmF/k/L6tL+pY9XPPONG7/YCZzA+A2qdv2HOLVcctpVbMsXRoGz5DnGuWKc80FcXw8fR2bC+jFVMHsjD5Bcc7tAiYAnYCNwLCcHw0HGuZpMpGzEBURSo/kBFIfa8M/Otdh0abdXDVwKnd8ONPT+44/M2oJR45n8eyVeT+g2N96/ToAeYXXUeR3fDkLJdbMSuZ8HwW0B5YCI4A2OS9rDWikhwSM6Igw/tq6Oql92tCnU23mbdjFlW9P4e7/zGLhxt35mmXisgy+X7CFB9rUIL5s0Xzdd16IKxXNLU2r8lXaxkJzg7Fg4csKvCIwwcwWALPIPgY+CngJuMbMFgIvAt39F1Pk7BQtEsa9l9QgtU8b/t6hFmnrfuGKtybT4+M0Fm/2f5EfOnqcp75dREJsUXq29t+AYn+7t011IkJD6D9e67RAYvl5b4mkpCSXlpaWb/sT+b09h47y4eS1DJm8mr2HjtGxfnkealeLuhVL+GV/r45dxlsTVvJZj6ZcXL2sX/aRX14Zs5SBE1fxw4OtqHeef94vOTUzm53zeeNvBOZVBCJ+UiIynF7tajL5sbb0urQmU1fuoPPrqdw7dDbLtu7N032tzNjLeymruPqCSkFf3gD3JFenRGQY/cZr9FqgUIFLoRQTFc7D7Wsx+bG2PNC2BinLt9Pp9RTu+2wOK7ade5E753hi+CKiI8J4Ip8GFPtbTHQ497Suzo/pGQF3imZhpQKXQi0mOpxHOtQmtU8b7r2kOhOXZtBhQAoPfj6XlRn7cv8Fp/HNnE3MXLOTf3SuQ9liRfIwsbfubBFP2WIRvDJGo9cCgQpcBChVNIJHO9Yh9bG23JNcnR/Tt9Gh/yQe/nLeGZ958cv+I7zwQzoXVi3FDUmV/ZTYG9ERYdzfpgYz1uxksgYge04FLnKS0kUj+EfnOqT2aUOPVgmMWbSVS1+byCNfzWfdDt+K/KXRS9l98CjPezSg2N9ualqFSiWj+LcGIHtOBS5yCmWKFeHxy+qS0qcNd7WoxqgFm2n72iQe/e981u84cNrtZq3dyZdpG+jeshp1KhTMMzWKhIXSq11NFmzczdjFwXUnyIJGpxGK+CBjzyHembSKoTPWk5XluPbCOO5rU4PKpaN/fc2RY1l0eTOV/YePM753MtERYR4m9q9jx7PoOCCFEDPGPJRMaAH8SyOQ6DRCkXNQrkQkfa+oT2qfNtzarCrD5myizasTeXzYwl8HLr8/eQ3Lt+3j6a71C3R5Q/YtfR/pUJsVGfsYMVcDkL2iFbjIWdiy+yADJ6ziy1kbcDiuPj+Ob+dvIrlmLINu/8NCqUDKynJ0fXsyuw4c5edHLiEiTOtBf9EKXCQPVYyJ4tluiUx89BKuT6rMsLkbCTGjb9fAGVDsbyEhxt871GbjLwf5ctZ6r+MUSlqBi+SBzbsOcuDIcWqUK+Z1lHzlnOOG96azZsd+Uh5tQ1REqNeRCiStwEX86LySUYWuvCFnAHKn2mTuPcxH09Z6HafQUYGLyDm5KL40bWrH8s7EVew+qAHI+UkFLiLn7JEOtdl98ChDUld7HaVQUYGLyDlLrBTD5Q0r8v7kNWzXAOR8owIXkTzRu30tDh09zsAJGoCcX3wZqRZpZjPNbL6ZLTazp3/38zfM7Oxv2yYiBUL12GJce2Ecn05f9+vFTeJfvqzADwNtnXONgMZAJzNrBmBmSUAp/8UTkWDSq10tAN78SQOQ80OuBe6ynVhhh+d8OTMLBf4N9PFjPhEJIpVKRnFLsyr8d/bGc7qfuvjGp2PgZhZqZvOADLKHGs8A7gdGOue25LJtTzNLM7O0zMzMcw4sIoHtvjY1iAoP5fr3pjFi7ibdctaPfCpw59xx51xjIA5oYmbJwHXAmz5sO8g5l+ScS4qNjT2nsCIS+MoWK8Kwey+mSuloHvpyHnf+Z5aOifvJGZ2F4pzbBUwA2gA1gJVmthaINrOVeZ5ORIJSrfLF+eZvF/OvLvWYsXonHfpN4qOpa8nK0mo8L/lyFkqsmZXM+T4KaA/Mds5VcM7FO+figQPOuRp+TSoiQSU0xLirZTXGPZzMBVVL0XfkYq57bxorM859aLRk82UFXhGYYGYLgFlkHwMf5d9YIlJQVC4dzcd3NaHf9Y1YlbmPy16fzBs/reDIsSyvowU93Y1QRPLN9n2H+b+Rixm1YAu1yxfn5Wsb0rhySa9jBTzdjVBEPFe2WBHeuvkChtyexO6DR7l64BSeHbWEA0eOeR0tKKnARSTftatXnnG9k7m5aRXen7yGjgNSSF2h04zPlApcRDxRIjKc57o14Kt7mhMeEsJt78/kka/ms+vAEa+jBQ0VuIh4qkm10vzQqxX3tanOiHmbaNdvEt8v2KILgHygAhcRz0WGh/Joxzp8d39LKsZEcd9nc+j5yWy27j7kdbSApgIXkYBR77wSDL/3Yp64rA6pKzJp328SQ2es0wVAp6ECF5GAEhYaQs/k6ox9KJnESjH8c/gibho8ndWZujnW76nARSQgVS1TlM96NOXlaxqwZMseOr2eysCJKzl6XBcAnaACF5GAZWbccFEVfurdmra1y/HKmGVc+dYUFm3a7XW0gKACF5GAV65EJO/ediHv3noBmfsOc+XbU3hxdDqHjh73OpqnVOAiEjQ6JVbkx4dbc+0Fcbw3aTWdBqQwbdUOr2N5RgUuIkElJjqcl69tyGfdm5Ll4KbB03l82AJ2HzzqdbR8pwIXkaB0cY2yjH0omZ7JCXw5awPt+01i7OKtXsfKVypwEQlaURGhPHFZXUbc14LSRSO455PZ3Dt0Nhl7C8cFQCpwEQl6DeNK8t0DLXm0Y21+TM+gfb8UvkrbUOAvx1eBi0iBEB4awn1tajC6Vytqly9On68XcOv7M1i/44DX0fzGl5FqkWY208zmm9liM3s65/mhZrbMzBaZ2QdmFu7/uCIif656bDG+6NmM57olMn/DbjoMmMTglNUcK4AXAPmyAj8MtHXONQIaA53MrBkwFKgDNACigO7+CikiciZCQoxbm1VlfO9kWtYoy/M/pHPVwKks3lywLgDKtcBdthM3IQjP+XLOuR9yfuaAmUCcH3OKiJyxijFRDL49ibduPp8tuw/S9a0pvDJmaYG5AMinY+BmFmpm84AMsocazzjpZ+HAbcCY02zb08zSzCwtM1MTN0Qkf5kZXRqex4+9W3P1+ZUYOHEVnV9PZfrq4L8AyKcCd84dd841JnuV3cTMEk/68UAgxTmXepptBznnkpxzSbGxseccWETkbJSMjuDf1zXi07ubcjzLceOg6Tw+bGFQXwB0RmehOOd2AROATgBm1heIBXrneTIRET9oWfPkC4DW077fJMYsCs4LgHw5CyXWzErmfB8FtAeWmll3oCNwk3Ou4H28KyIF1okLgL69ryVlihXhr5/O5q+fzCZjT3BdAOTLCrwiMMHMFgCzyD4GPgp4FygPTDOzeWb2Lz/mFBHJcw3iYhh5fwv6dKrNz8syuLTfJL6YuT5oLgCy/AyalJTk0tLS8m1/IiK+WrN9P//4ZgEz1uykWUJpXry6IdXKFvU6FgBmNts5l/T753UlpogIUK1sUT7v0YyXrm7A4s176DQghXcmrgroCUAqcBGRHCEhxo1NqvBj79a0qV2Ol8cs5cq3prBwY2BeAKQCFxH5nfInTQDavu8wV749mRd+SOfgkcC6AEgFLiJyGp0SKzK+d2tuuKgyg1JW03FAClNWbvc61q9U4CIifyImKpwXr27I5z2aERpi3DJkBn2+ns/uA95fAKQCFxHxQfPqZRjdqxX3XlKdb+Zs4tJ+k/h+wRZPTzlUgYuI+CgyPJQ+neow8v4WVIyJ5L7P5tDzk9ls3e3NBUAqcBGRM1T/vBiG33sxT1xWh9QVmbTvN4lPp68jKyt/V+MqcBGRsxAWGkLP5OqMfSiZhpVjeHLEIm4cNJ1Vmfty3ziPqMBFRM5B1TJF+fTuprxybUOWbdtL5wGpvPXzCo4c8/8FQCpwEZFzZGZcn1SZ8b2TaV+/PK+OW07XtyYzb8Muv+5XBS4ikkfKFY/k7ZsvYPDtSew6cJSrB07h2VFLOHDkmF/2pwIXEclj7euVZ1zvZG5uWoX3J6+hQ/8Ulm3dm+f7UYGLiPhBichwnuvWgP/+tTkJscWIKxWV5/sIy/PfKCIiv7oovjQf39XEL79bK3ARkSDly0i1SDObaWbzzWyxmT2d83w1M5thZivN7Eszi/B/XBEROcGXFfhhoK1zrhHQGOhkZs2Al4H+zrkawC/A3X5LKSIif5BrgbtsJy4tCs/5ckBb4Ouc5z8CuvkjoIiInJpPx8DNLNTM5gEZwHhgFbDLOXfi5MaNQKXTbNvTzNLMLC0zMzMPIouICPhY4M654865xkAc0ASo4+sOnHODnHNJzrmk2NjYs0spIiJ/cEZnoTjndgETgOZASTM7cRpiHLApb6OJiMif8eUslFgzK5nzfRTQHkgnu8ivzXnZX4Bv/ZRRREROwXKbJmFmDcn+kDKU7ML/yjn3jJklAF8ApYG5wK3OucO5/K5MYN1ZZi0LBM4wOu/p/fgfvRe/pffjtwrC+1HVOfeHY9C5FnigMLM051yS1zkChd6P/9F78Vt6P36rIL8fuhJTRCRIqcBFRIJUMBX4IK8DBBi9H/+j9+K39H78VoF9P4LmGLiIiPxWMK3ARUTkJCpwEZEgFRQFbmadzGxZzq1r/+F1Hq+YWWUzm2BmS3Ju7dvL60yBIOdePXPNbJTXWbxmZiXN7GszW2pm6WbW3OtMXjGzh3P+nSwys8/NLNLrTHkt4AvczEKBt4HOQD3gJjOr520qzxwDHnHO1QOaAfcV4vfiZL3IvjpY4HVgjHOuDtCIQvq+mFkl4EEgyTmXSPaFiDd6myrvBXyBk33zrJXOudXOuSNkX/15pceZPOGc2+Kcm5Pz/V6y/3Ge8i6QhYWZxQGXA0O8zuI1M4sBkoH3AZxzR3LuX1RYhQFROfdsigY2e5wnzwVDgVcCNpz0+LS3ri1MzCweOB+Y4XEUrw0A+gBZHucIBNWATODDnENKQ8ysqNehvOCc2wS8CqwHtgC7nXPjvE2V94KhwOV3zKwY8A3wkHNuj9d5vGJmXYAM59xsr7MEiDDgAuAd59z5wH6gUH5mZGalyP5LvRpwHlDUzG71NlXeC4YC3wRUPulxob51rZmFk13eQ51zw7zO47EWQFczW0v2obW2Zvapt5E8tRHY6Jw78VfZ12QXemHUDljjnMt0zh0FhgEXe5wpzwVDgc8CauYMUY4g+4OIkR5n8oSZGdnHN9Odc/28zuM159zjzrk451w82f9d/OycK3CrLF8557YCG8ysds5TlwJLPIzkpfVAMzOLzvl3cykF8APdsNxf4i3n3DEzux8YS/YnyR845xZ7HMsrLYDbgIU5I+4AnnDO/eBdJAkwDwBDcxY7q4E7Pc7jCefcDDP7GphD9tlbcymAl9TrUnoRkSAVDIdQRETkFFTgIiJBSgUuIhKkVOAiIkFKBS4iEqRU4CIiQUoFLiISpP4fLOafT0lBmEUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-firewall",
   "metadata": {},
   "source": [
    "This code calculates the accuracy of the model.\n",
    "It used sklearn accuracy_score class.\n",
    "\n",
    "1. torch.no_grad() -> It is used while predicting the outcomes because we are not going to optimize any weights here.\n",
    "2. For every batch, we calculate the output and find out the assiciated labels.\n",
    "3. Then we find the largest value from the predictions and use that in the accuracy_score class to calculate the accuracy of the model.\n",
    "\n",
    "It calculates the accuracy of every mini batch and stores it in an array.\n",
    "Then we find the mean of the array to calculate the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "first-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7010841836734694\n"
     ]
    }
   ],
   "source": [
    "#Predicting the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = []\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(dataloader):\n",
    "        output = net(image)\n",
    "        softmax = torch.exp(output)\n",
    "        prob = list(softmax.numpy())\n",
    "        pred = np.argmax(prob, axis=1)\n",
    "        accr = accuracy_score(label, pred)\n",
    "        accuracy.append(accr)\n",
    "        \n",
    "print(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-split",
   "metadata": {},
   "source": [
    "It is another custom dataset class but for test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "native-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset2(Dataset):\n",
    "    def __init__(self, label_file, image_path, transform=None):\n",
    "        self.labels = pd.read_csv(label_file)\n",
    "        self.image_path = image_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_path, f'{self.labels.iloc[index, 0]}.png')\n",
    "        image = io.imread(img_path).astype('float32')\n",
    "        image /= 255.0\n",
    "        image = np.array(image)\n",
    "        image = image.reshape(3, 64, 64)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "suspended-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image = sample\n",
    "        image = torch.from_numpy(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "swiss-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset2(label_file='example.csv', image_path='test/', transform=transforms.Compose([ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "south-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader2 = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "convinced-investing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 3, 64, 64])\n",
      "1 torch.Size([64, 3, 64, 64])\n",
      "2 torch.Size([64, 3, 64, 64])\n",
      "3 torch.Size([64, 3, 64, 64])\n",
      "4 torch.Size([64, 3, 64, 64])\n",
      "5 torch.Size([64, 3, 64, 64])\n",
      "6 torch.Size([16, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for i, image in enumerate(dataloader2):\n",
    "    print(i, image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-priest",
   "metadata": {},
   "source": [
    "This code will predict the associated class for the mini batch images created above and store it in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "convinced-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = test_dataset.labels['Id']\n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, image in enumerate(dataloader2):\n",
    "        output = net(image)\n",
    "        \n",
    "        softmax = torch.exp(output)\n",
    "        prob = np.array(softmax.numpy())\n",
    "        prediction = np.argmax(prob, axis=1)\n",
    "        \n",
    "        pred.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "amino-competition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-4e7d89985bcb>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  pred = np.array(pred)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.array(pred)\n",
    "type(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-engagement",
   "metadata": {},
   "source": [
    "The class we got are numerical or encoded. Here we decode the class and store it in another list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "personalized-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "\n",
    "for i in range(pred.shape[0]):\n",
    "    for j in range(pred[i].shape[0]):\n",
    "        if pred[i][j] == 0:\n",
    "            pred_labels.append('Connective')\n",
    "        elif pred[i][j] == 1:\n",
    "            pred_labels.append('Immune')\n",
    "        elif pred[i][j] == 2:\n",
    "            pred_labels.append('Cancer')\n",
    "        elif pred[i][j] == 3:\n",
    "            pred_labels.append('Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "confirmed-region",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cancer',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Cancer',\n",
       " 'Connective',\n",
       " 'Cancer',\n",
       " 'Immune',\n",
       " 'Cancer',\n",
       " 'Cancer']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-comparison",
   "metadata": {},
   "source": [
    "This code will generate a csv file which contains the predictions and the image name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "offshore-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'Id': names,\n",
    "    'Type': pred_labels\n",
    "}\n",
    "\n",
    "dataframe = pd.DataFrame(output, columns=['Id', 'Type'])\n",
    "dataframe = dataframe.sort_values('Id')\n",
    "\n",
    "dataframe.to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-briefs",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
